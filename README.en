
The goal of these exercises is to familiarize yourself with job submissions on
a cluster. To do that, we will use a home-made image processing tool called
"filterImage.exe" and a set of images.

===== Import the Set of Images =====
For some exercises, we will use a dataset made of high-resolution pictures.
If not done already, you must use Globus to import all files to your home
directory.

- Go to globus.computecanada.ca and log in
- Select the source endpoint. For example: intro-arc_guillimin_plstonge
- Select a cluster endpoint. For example: computecanada#guillimin
- Import the "pictures" folder

===== Compiling filterImage.exe ======
The image processing tool must be built (compiled). To do that, you first have
to load the appropriate modules. The compilation requires a GCC or Intel
compiler, OpenMPI, the BOOST library and the Image Magick library.
You can find the appropriate modules with the following commands:

   module avail
   module spider boost

On Colosse, you must load the following modules:

   module load compilers/gcc libs/boost libs/image_magick

On Guillimin, you must load the following modules:

   module load iomkl/2015b Boost/1.59.0-Python-2.7.10 ImageMagick/7.0.1-6

When all modules are loaded, you can compile the executable with the command:

   make

This will create a file named filterImage.exe in your directory. This
executable will be used for each exercise.

===== Outline of Exercises =====
Each exercise is located in its own directory. Each exercise has a
README.en file, a submit.sh file that you will edit, and a solution.sh file.

Try exercises according to your needs:

  * 1-base
    Getting started with job submission: useful for any use case.

  * 2-sequentielles-multiples
    This exercise is useful if you run multiple serial jobs running for hours

  * 3-gnu-parallel
    This exercise is useful if you run multiple short serial jobs (<1 hour)

  * 4-lot-de-taches
    This exercise is useful if you run hundreds of jobs; you need job arrays

  * 6-tache-mpi
    This exercise is useful if you run parallel jobs with MPI on multiple nodes
